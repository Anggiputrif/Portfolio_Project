{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'train1.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(a):\n",
    "  if a == 'Non Negatif' :\n",
    "    return 0\n",
    "  else :\n",
    "    return -1\n",
    "\n",
    "df['Aktual'] = df['Sentimen'].apply(label)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CASE FOLDING\n",
    "df[\"Case_Folding\"] = df[\"Comment\"].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT EMOJI\n",
    "emoji_dict = {\n",
    "    \"😊\": \"bahagia\", \"😢\": \"sedih\", \"🥲\": \"tersentuh\", \"😓\": \"sedih\",\n",
    "    \"❤️\": \"suka\", \"❤\": \"suka\", \"👍\": \"suka\", \"🫰\": \"suka\", \"💗\": \"suka\",\n",
    "    \"👎\": \"tidak suka\", \"🎉\": \"meriah\", \"🤔\": \"berpikir\", \"😂\": \"tertawa\",\n",
    "    \"🤣\": \"tertawa\", \"😍\": \"terpesona\", \"🙌\": \"pujian\", \"😭\": \"menangis\",\n",
    "    \"🤲\": \"harapan\", \"😔\": \"sedih\", \"😌\": \"sedih\", \"😞\": \"sedih\", \"✅\": \"iya\",\n",
    "    \"✔️\": \"iya\", \"❌\": \"tidak\", \"🔥\": \"keren\", \"✨\": \"dukungan\", \"✔\": \"iya\",\n",
    "    \"😳\": \"takjub\", \"🤩\": \"takjub\", \"🥺\": \"memohon\", \"😩\": \"lelah\",\n",
    "    \"😁\": \"antusias\", \"😎\": \"keren\", \"😄\": \"bahagia\", \"🤭\": \"bercanda\",\n",
    "    \"🙏\": \"harapan\", \"🙏🏻\": \"harapan\", \"🫢\": \"bungkam\", \"😅\": \"canggung\",\n",
    "    \"🥰\": \"bahagia\", \"👏\": \"apresiasi\", \"👏🏻\": \"apresiasi\", \"👏🏽\": \"apresiasi\",\n",
    "    \"😻\": \"suka\", \"🫡\": \"bangga\", \"😃\": \"bahagia\", \"😲\": \"takjub\",\n",
    "    \"😆\": \"gembira\", \"💖\": \"suka\", \"💛\": \"suka\", \"💙\": \"suka\", \"💕\": \"suka\",\n",
    "    \"💓\": \"suka\", \"♥️\": \"suka\", \"🤍\": \"suka\", \"🖤\": \"suka\", \"👍🏻\": \"suka\",\n",
    "    \"🤯\": \"takjub\", \"💯\": \"keren\", \"👍🏽\": \"suka\", \"🥹\": \"bangga\",\n",
    "    \"😏\": \"sindiran\", \"😒\": \"kesal\", \"😫\": \"kesal\", \"🤗\": \"terimakasih\",\n",
    "    \"👏🏼\": \"apresiasi\", \"👍🏻\": \"suka\", \"🫶\": \"suka\"}\n",
    "\n",
    "# Membuat fungsi untuk mengubah emoji ke teks\n",
    "def convert_emoji(text):\n",
    "    for emoji, replacement in emoji_dict.items():\n",
    "        text = text.replace(emoji, replacement)\n",
    "    return text\n",
    "\n",
    "# Mengganti emoji dalam kolom full_text\n",
    "df['Convert_Emoji'] = df['Case_Folding'].apply(convert_emoji)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANSING\n",
    "def cleansing(text):\n",
    "#menghapus url\n",
    "  text = re.sub(r'(https?://\\S+|www\\.\\S+)|(www\\.[^\\s]+)',' ', text)\n",
    "#menghapus hashtag\n",
    "  text = re.sub(r'#([^\\s]+)',' ', text)\n",
    "#menghapus username\n",
    "  text = re.sub(r'@([^\\s]+)',' ', text)\n",
    "#menghapus punctuation\n",
    "  text = re.sub(r'[!$%^&*@#()_+±|~=`{}\\[\\]%\\-:\";\\'<>‘’?,.“”\\/]',' ', text)\n",
    "#menghapus digit\n",
    "  text = re.sub(r'\\d+',' ',text)\n",
    "#menghapus whitespace\n",
    "  text = re.sub(r'[\\s]+',' ', text)\n",
    "  return text\n",
    "\n",
    "df['Cleansing'] = df['Convert_Emoji'].apply(cleansing)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZING\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenizing(text):\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  return tokens\n",
    "\n",
    "df['Tokenizing'] = df['Cleansing'].apply(tokenizing)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = 'colloquial-indonesian-lexicon.csv'\n",
    "normalized_lexicon = pd.read_csv(file_path1)\n",
    "normalized_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path2 = 'normalisasi.xlsx'\n",
    "normal_word = pd.read_csv(file_path2)\n",
    "normal_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_word = pd.concat([normalized_lexicon, normal_word], ignore_index=True)\n",
    "normalized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZATION\n",
    "normalized_word_dict = {}\n",
    "\n",
    "for index, row in normalized_word.iterrows():\n",
    "    if row[0] not in normalized_word_dict:\n",
    "        normalized_word_dict[row[0]] = row[1]\n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalized_word_dict[term] if term in normalized_word_dict else term for term in document]\n",
    "\n",
    "df['Normalization'] = df['Tokenizing'].apply(normalized_term)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOPWORD REMOVAL\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "list_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negation_word = ['belum','belumlah','bukan','bukankah','bukanlah','bukannya','jangan',\n",
    "                 'jangankan','janganlah','tanpa','tidak','tidakkah','tidaklah']\n",
    "\n",
    "for word in list(list_stopwords):\n",
    "  if word in negation_word:\n",
    "    list_stopwords.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stopwords.extend(['hmmm', 'eh', 'kah', 'hi', 'wuih', 'nih', 'nya', 'coy', 'ya', 'gurls', 'dah', 'ah', 'wkwkwk', 'wkwkwkw', 'ni', 'nihhhhhhh', 'nah', 'di', 'si', 'aaaaaa', 'deh', 'cuy', 'gw', 'hehe', 'dsb', 'gue', 'bilamana', 'ber',\n",
    "                       'kan', 'w', 'wk', 'mb', 'hahahha', 'e', 'pfftttt', 'yuk', 'sih', 'nih', 'lo', 'naah', 'nii', 'jn', 'ini', 'hua', 'ah', 'slurr', 'anjay', 'ny', 'kak', 'min', 'guyss', 'kakkk', 'wkwkwk…', 'sok', 'teng', 'aaaakkk',\n",
    "                       'mimin','dll', 'plis', 'yaa', 'xd', 'an', 'lu', 'tb', 'yo', 'lohh', 'dehh', 'o', 'r', 'y', 'ehhh', 'wkwkwkwkwk', 'f', 'wkwk', 'guys', 'ke', 'ok', 'siih', 'kokkk', 'ta', 'oh', 'btwww', 'plisss', 'tent', 'duh',\n",
    "                       'aaah', 'heh', 'sis', 'aaaaa', 'waaa', 'ngl', 'ohh', 'bant', 'wakakaka', 'ama', 'kaa', 'kakk', 'kaka', 'ka', 'lohh', 'ihh', 'hah', 'kaaa', 'niiiiiiiii', 'huhuuu', 'gpp', 'pliss', 'hehe', 'gais', 'srg', 'nge', 'aco',\n",
    "                       'dllnya', 'wiiihh', 'anjaaay', 'loh', 'woi', 'woy', 'aaaaaaaa', 'huhuhu', 'wkwkwkk', 'huwaaaaa', 'fallawwww', 'fallawwwwwww', 'hai', 'ha', 'haha', 'woooooooo', 'hayooh', 'xixixi', 'yhh', 'lololo', 'ges', 'wkwkwkwk',\n",
    "                       'nahh', 'xixi', 'wkwkkwk', 'wkkwkwk', 'lho', 'huhu', 'wkwkw', 'btw', 'wkwkwk', 'awokawok', 'hehehe', 'oi', 'awowkowkok', 'huhuhuu', 'ygy', 'yagesya', 'wkkwkw', 'wkwkkww', 'nnya', 'alah', 'cok', 'gass', 'halo',\n",
    "                       'wkwkwk', 'donh', 'ddos', 'tt', 'bro', 'rw', 'tenggg', 'halah', 'dooonk', 'mah', 'fyp', 'bgst', 'ih', 'huahaha', 'a', 'p', 't', 'h', 'onoo', 'b', 'n', 'dot', 'id'])\n",
    "\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "df['Stopword_Removal'] = df['Normalization'].apply(stopwords_removal)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nlp-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEMMATIZATION\n",
    "from nlp_id.lemmatizer import Lemmatizer\n",
    "\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "  if isinstance(text, list):\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(str(token)) for token in text]\n",
    "    return lemmatized_tokens\n",
    "  else:\n",
    "    return str(text)\n",
    "\n",
    "df['Lemmatization'] = df['Stopword_Removal'].apply(lemmatize)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Lemmatization']\n",
    "y = df['Aktual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#SPLIT DATA TESTING DAN DATA TRAINING 90:10\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.astype('U'), y, test_size=0.1, random_state=42)\n",
    "print(f'Total training data : {len(X_train)}')\n",
    "print(f'Total testing data : {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT DATA TESTING DAN DATA TRAINING 80:20\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X.astype('U'), y, test_size=0.2, random_state=42)\n",
    "print(f'Total training data : {len(X_train2)}')\n",
    "print(f'Total testing data : {len(X_test2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT DATA TESTING DAN DATA TRAINING 70:30\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X.astype('U'), y, test_size=0.3, random_state=42)\n",
    "print(f'Total training data : {len(X_train3)}')\n",
    "print(f'Total testing data : {len(X_test3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer()\n",
    "#TF-IDF 90:10\n",
    "tfidf_train = tf.fit_transform(X_train)\n",
    "tfidf_test = tf.transform(X_test)\n",
    "print(tfidf_train,'\\n')\n",
    "print(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF 80:20\n",
    "tfidf_train2 = tf.fit_transform(X_train2)\n",
    "tfidf_test2 = tf.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF 70:30\n",
    "tfidf_train3 = tf.fit_transform(X_train3)\n",
    "tfidf_test3 = tf.transform(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NB_classifier = MultinomialNB()\n",
    "#TRAINING MODEL MENGGUNAKAN DATA LATIH 90:10\n",
    "classifier = NB_classifier.fit(tfidf_train, y_train)\n",
    "#PREDIKSI PADA DATA UJI\n",
    "y_pred = classifier.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING MODEL MENGGUNAKAN DATA LATIH 80:20\n",
    "classifier2 = NB_classifier.fit(tfidf_train2, y_train2)\n",
    "#PREDIKSI PADA DATA UJI\n",
    "y_pred2 = classifier.predict(tfidf_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING MODEL MENGGUNAKAN DATA LATIH 70:30\n",
    "classifier3 = NB_classifier.fit(tfidf_train3, y_train3)\n",
    "#PREDIKSI PADA DATA UJI\n",
    "y_pred3 = classifier.predict(tfidf_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "#CONFUSION MATRIX\n",
    "cnf_matrix1 = confusion_matrix(y_test,y_pred)\n",
    "cnf_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix2 = confusion_matrix(y_test2,y_pred2)\n",
    "cnf_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix3 = confusion_matrix(y_test3,y_pred3)\n",
    "cnf_matrix3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report 90:10\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report 80:20\n",
    "print(classification_report(y_test2,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report 70:30\n",
    "print(classification_report(y_test3,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['TN','FP','FN','TP']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix3.flatten()]\n",
    "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cnf_matrix3, annot=labels, fmt='', cmap='Blues');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame({'Lemmatization': X_train3, 'Label': y_train3})\n",
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train = X_train_df[X_train_df['Label'] == -1]\n",
    "neg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df = pd.DataFrame({'Lemmatization': X_test3, 'Label': y_test3})\n",
    "X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelling=[]\n",
    "for i, text in enumerate(X_test3):\n",
    "  labelling.append({\"Lemmatization\" : text, \"Label\": y_pred3[i]})\n",
    "\n",
    "labelling_output = pd.DataFrame(labelling)\n",
    "print(labelling_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_test = labelling_output[labelling_output['Label'] == -1]\n",
    "neg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'test1.xlsx'\n",
    "df_test = pd.read_excel(file_path)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CASE FOLDING\n",
    "df_test[\"Case_Folding\"] = df_test[\"Comment\"].str.lower()\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT EMOJI\n",
    "emoji_dict = {\n",
    "    \"😊\": \"bahagia\", \"😢\": \"sedih\", \"🥲\": \"tersentuh\", \"😓\": \"sedih\",\n",
    "    \"❤️\": \"suka\", \"❤\": \"suka\", \"👍\": \"suka\", \"🫰\": \"suka\", \"💗\": \"suka\",\n",
    "    \"👎\": \"tidak suka\", \"🎉\": \"meriah\", \"🤔\": \"berpikir\", \"😂\": \"tertawa\",\n",
    "    \"🤣\": \"tertawa\", \"😍\": \"terpesona\", \"🙌\": \"pujian\", \"😭\": \"menangis\",\n",
    "    \"🤲\": \"harapan\", \"😔\": \"sedih\", \"😌\": \"sedih\", \"😞\": \"sedih\", \"✅\": \"iya\",\n",
    "    \"✔️\": \"iya\", \"❌\": \"tidak\", \"🔥\": \"keren\", \"✨\": \"dukungan\", \"✔\": \"iya\",\n",
    "    \"😳\": \"takjub\", \"🤩\": \"takjub\", \"🥺\": \"memohon\", \"😩\": \"lelah\",\n",
    "    \"😁\": \"antusias\", \"😎\": \"keren\", \"😄\": \"bahagia\", \"🤭\": \"bercanda\",\n",
    "    \"🙏\": \"harapan\", \"🙏🏻\": \"harapan\", \"🫢\": \"bungkam\", \"😅\": \"canggung\",\n",
    "    \"🥰\": \"bahagia\", \"👏\": \"apresiasi\", \"👏🏻\": \"apresiasi\", \"👏🏽\": \"apresiasi\",\n",
    "    \"😻\": \"suka\", \"🫡\": \"bangga\", \"😃\": \"bahagia\", \"😲\": \"takjub\",\n",
    "    \"😆\": \"gembira\", \"💖\": \"suka\", \"💛\": \"suka\", \"💙\": \"suka\", \"💕\": \"suka\",\n",
    "    \"💓\": \"suka\", \"♥️\": \"suka\", \"🤍\": \"suka\", \"🖤\": \"suka\", \"👍🏻\": \"suka\",\n",
    "    \"🤯\": \"takjub\", \"💯\": \"keren\", \"👍🏽\": \"suka\", \"🥹\": \"bangga\",\n",
    "    \"😏\": \"sindiran\", \"😒\": \"kesal\", \"😫\": \"kesal\", \"🤗\": \"terimakasih\",\n",
    "    \"👏🏼\": \"apresiasi\", \"👍🏻\": \"suka\", \"🫶\": \"suka\"\n",
    "}\n",
    "\n",
    "# Membuat fungsi untuk mengubah emoji ke teks\n",
    "def convert_emoji(text):\n",
    "    for emoji, replacement in emoji_dict.items():\n",
    "        text = text.replace(emoji, replacement)\n",
    "    return text\n",
    "\n",
    "# Mengganti emoji dalam kolom full_text\n",
    "df_test['Convert_Emoji'] = df_test['Case_Folding'].apply(convert_emoji)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANSING\n",
    "def cleansing(text):\n",
    "#menghapus url\n",
    "  text = re.sub(r'(https?://\\S+|www\\.\\S+)|(www\\.[^\\s]+)',' ', text)\n",
    "#menghapus hashtag\n",
    "  text = re.sub(r'#([^\\s]+)',' ', text)\n",
    "#menghapus username\n",
    "  text = re.sub(r'@([^\\s]+)',' ', text)\n",
    "#menghapus punctuation\n",
    "  text = re.sub(r'[!$%^&*@#()_+±|~=`{}\\[\\]%\\-:\";\\'<>‘’?,.“”\\/]',' ', text)\n",
    "#menghapus digit\n",
    "  text = re.sub(r'\\d+',' ',text)\n",
    "#menghapus whitespace\n",
    "  text = re.sub(r'[\\s]+',' ', text)\n",
    "  return text\n",
    "\n",
    "df_test['Cleansing'] = df_test['Convert_Emoji'].apply(cleansing)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZING\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenizing(text):\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  return tokens\n",
    "\n",
    "df_test['Tokenizing'] = df_test['Cleansing'].apply(tokenizing)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZATION\n",
    "normalisasi_kata_dict = {}\n",
    "\n",
    "for index, row in normalized_word.iterrows():\n",
    "    if row[0] not in normalisasi_kata_dict:\n",
    "        normalisasi_kata_dict[row[0]] = row[1]\n",
    "\n",
    "def normalisasi_kata(document):\n",
    "    return [normalisasi_kata_dict[term] if term in normalisasi_kata_dict else term for term in document]\n",
    "\n",
    "df_test['Normalization'] = df_test['Tokenizing'].apply(normalisasi_kata)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOPWORD REMOVAL\n",
    "df_test['Stopword_Removal'] = df_test['Normalization'].apply(stopwords_removal)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEMMATIZATION\n",
    "df_test['Lemmatization'] = df_test['Stopword_Removal'].apply(lemmatize)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest_selected = df_test[['Id', 'Comment', 'Stopword_Removal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_combined = [' '.join(tokens) for tokens in df_test['Lemmatization']]\n",
    "tfidf_newtest = tf.transform(text_combined)\n",
    "y_pred_new = classifier3.predict(tfidf_newtest)\n",
    "label=[]\n",
    "for i, text in enumerate(df_test['Lemmatization']):\n",
    "    label.append({\"Lemmatization\": text, \"Label\": y_pred_new[i]})\n",
    "\n",
    "output = pd.DataFrame(label)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([dfTest_selected, output], axis=1)\n",
    "df_combined"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
